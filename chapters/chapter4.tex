
\chapter{Hand gesture Recognition} \label{hgr}




\section{Hand tracking  And Detection}

\subsection{Hand segmentation based on depth information :} \label{depth}

The First step we need to accomplish in order to start to work with the depth data, is to decide which pixels are we going to take into account to carry on the tracking. The Kinect can catch the distance of the points which are visible to the camera, between the values \textbf{minDistance and maxDistance} (\ref{fig:cam8}).

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{img/mindistance.png}
\caption{Relative depth thresholding }
\label{fig:cam8}
\end{figure}

We are going to base our choice on the object’s proximity to the Kinect. We can choose if a pixel is near based on its depth  The lesser the depth the more likely it’s the hand region .\\

\textbf{ Absolute depth}: A pixel is near if its depth is lesser than a constant value, that means that the pixel is between \textbf{minDistance }and a predefined value which is greater than \textbf{minDistance }and lesser than \textbf{maxDistance}.\\

\textbf{Relative depth }:First of all,  we calculate the minimum depth, and  base on that depth we select a maximum depth (by adding a constant value to the minimum depth). So, if the depth is between these two values, the pixel is near. This method allow us to have greater mobility, because it does not force us to stay in the same position the whole time. 
\\


\textbf{In the figure \ref{fig:cam8}} the minimum depth is \textbf{minDistance} and we add a tolerance of\textbf{  t= 2cm }to it, so every point between \textbf{minDistance and minDistance + t }  will be accepted as near.\\

After selecting a method, we must select the resolution of the depth image. It is possible to select it from three different  options 80x60, 320x240 or 640x480. The best option is obviously the 640x480 resolution, but in order to improve the efﬁciency of the code, we can choose the 271 x 211 resolution, because it gives us enough deﬁnition to distinguish the contour  of the hands. \\This choice greatly reduce the number of operations, and consequently, it will improve the efﬁciency of the code.

\subsection{Hand Coordinates in depth space and color space }
A normal color camera can be used for sign language recognition but the problem comes when we want to use it for real time applications, in this case we have to track the hand position first and then we can go for recognition but implementing the hand tracking will involve complex algorithms which will make the overall system  computationally heavy, but with the kinect depth camera this task becomes easy as it has a skeleton tracking capability by using color and depth images so we can track the approximate hand position using the skeleton data, there are 20 joints that can be tracked these joints numbers are shown in \ref{table:t1}\\

\begin{table}[h!]
\centering

\begin{tabu} to 0.6\textwidth { | X[l] | X[r] | }

 \hline
 SpineMid = 1  & Neck = 2    \\
 \hline 
 Head = 3  &  ShoulderLeft = 4    \\
\hline
  ElbowLeft = 5   &  WristLeft = 6    \\
\hline

 HandLeft =	7  &   ShoulderRight = 8    \\
\hline
 
ElbowRight = 9  &  WristRight = 10   \\
\hline
 
 HandRight = 11  &  HipLeft = 12   \\
\hline
 
 KneeLeft = 13  &  AnkleLeft = 14   \\
\hline
 
FootLeft = 15  &  HipRight = 16   \\
\hline
 
 KneeRight = 17  & AnkleRight = 18     \\
\hline

FootRight = 19   & SpineShoulder = 20     \\

\hline

 \end{tabu}
\caption{JointType Enumeration}
 \label{table:t1} 

 \end{table}

in the following figure \ref{fig:cam9} we will show the hand position both in color and depth space :

\begin{figure}[H]
\centering
\includegraphics[width= 1.0\textwidth]{img/colorvsdepth.PNG}

\caption {Hand position's in both color image and depth imge .
\label{fig:cam9}}

\end{figure}

\subsection{Hand isolation using object connectivity }

After depth thresholding we notice that there are regions that do not belong to the hand , such as what appears to be one of my  arms or some furniture or my body when my hand is closer to my body like shown in the figure  \ref{fig:cam10} . These objects just happen to be on the same depth layer as my arm and hand this means that the Distance between these objects and the Kinect lies  between \textbf{ $$[minDistance ,minDistance +2cm] \pm {Error\ committed\ by\ Kinect\ } $$ } .


\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{img/depththresholding.png}
\caption{the depth of the body is in  the range [minDistance ,minDistance +2cm]}
\label{fig:cam10}
\end{figure}

the approach used in this project  is to realize that most of the times, hands are not connected to any other object after the thresholding . We already know that the center region belongs to the hand from skeleton tracking , so we can simply apply`` floodfill ()`` offered by OpenCV to find all the connected image regions.\newline

\textbf{ Definition and explanation of the algorithm used in floodfill () }

\begin{algorithm}

\caption{Algorithm of Flood Fill }

\textbf{function}  Flood-fill(node,target\_color,replacement\_color) 
\newline
\If{target\_color  ==  replacement\_color }
    {
     return
    }
\newline
\If{color of node \neq target\_color }
    {
     return
    }
Set the color of node to replacement$\_$color 


\textbf{Flood-fill}(one step to the west of node , target\_color , replacement\_color)\\
\textbf{Flood-fill}(one step to the east of node , target\_color , replacement\_color)\\
\textbf{Flood-fill}(one step to the north of node , target\_color , replacement\_color)\\
\textbf{Flood-fill}(one step to the south of node , target\_color , replacement\_color)
\newline

\textbf{return.} 
\end{algorithm}
\newline

with this simple algorithm we can seperate objects from each other and therefor the following  Results : \\
 
 \begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{img/finalresult.png}
\caption{result after applying connectivity check for one hand  }
\label{fig:cam11}
\end{figure}


the same process done with the other hand , Making a \textbf{OR } operation to the two images we end up having two hands isolated like shown in fig \ref{fig:twohd}

 \begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{img/twohands.jpg}
\caption{result after applying connectivity for both hands  }
\label{fig:twohd}
\end{figure}

now after finishing  the prepocessing phase , Since we are taking a resolution of 271 $\times$ 211 Pixels , to make sure the hands are being tracked we make a bounding box that is basically related to the left hand Position , this was made under the assumption that sign language  usually performed with two hands close to each other , \\
bounding box steps : \\

\begin{itemize}
\item  Use LEFT hand location provided by SDK Skeleton  tracker (Px, Py)
 \item Crop to box around hand using depth at (Px, Py)
    \item scale = $\frac{20000.0f}{Depth(px,py)}$ 
    \begin{enumerate}
    \item Constant 20000.0f  found empirically
    \item Proportional to the area of bounding box 
    \end{enumerate}
\item Top left Pixel  = ( Px -10 * Scale * 0.5  , Py - 10*Scale * 0.5)
\begin{enumerate}
    \item This formula scales an initial 10x10 box centered at (Px, Py) to
the appropriate dimensions based on the depth 
    \item Constants 0.5  take into account that most hand
silhouettes are more tall than wide 
    \end{enumerate}
   \item Width = Height = 10 * Scale 
\end{itemize}

in the next section we will build the recognition System  .

\section{ gesture recognition using Surf  Features  and SVM  classifier }

\subsection{Feature Descriptor selection :}
Visual gesture
recognition becomes a research hotspot in machine learning
in recent years, some recognition gesture models consider the
distribution of the palm and fingers structure as the spatial
characteristics of the gesture, which shows robustness and accuracy
when facing image rotation, scale invariance.\\ Geometric
invariant moments are often characterized as an apparent
features of gesture image, but they have to consume a lot of
memory and confront slow calculation and other shortcomings.\\ HOG(histograms of oriented gradients) feature has been introduced into gesture
recognition, but the HOG features is not ideal for gesture
recognition for it is computation in dense grids at some single
scale without orientation alignment.\\ The contribution of this project is that we propose SURF
feature descriptors of hand gesture images and then bag of
visual words are to map these descriptors to a dimension vector and a  support vector machine(SVM) classifier is then  trained to
recognize hand gestures.
\begin{enumerate}
\item We show that this model is not only
effective for common hand gestures dataset, but also achieves
good  recognition rate for depth
data collected by Kinect sensor.
\item Hand gestures training images can be represented by sets of
keypoint descriptors generated by SIFT or SURF , but the numbers of keypoints from the
images are \textbf{different and lack meaningful ordering }. This
creates difficulties for machine learning methods . To address
this problem, we use the bag-of-words ( also called bag of features )  approach .
\item We found
that SURF feature descriptors are more appropriately for visual
hand gesture recognition than SIFT feature descriptors.
SIFT is  slow which  gave lag during feature extraction. on the other hand SURF performs well on blurry images and is typically 3 times faster than SIFT.\\ Hence we found SURF descriptors to be apt for our application to meet the requirement  \textbf{of a  Real time application} since Sift descriptor can not be used in real time applications however Not with normal machines .
\end{enumerate}


\subsection{  Bag Of visual Words Model}

 \textbf{Bag-of-words (BoW) } model is one of the most popular approaches in this computer vision, The idea is very similar to a Dictionary of keypoints or local interest points of an Image or a Datasets of Images.We represent an object as a bag of visual words Fig \ref{fig:coodbook}.
\begin{wrapfigure}{l}{0.25\textwidth}
\includegraphics[width=0.25\textwidth]{img/codebook.jpg}
\caption{Example of Visual words in a codebook}
\label{fig:coodbook}
\end{wrapfigure}
These visual words are basically important points in the images. These points are \textbf{features}, and they are discriminative. What this means is that a big patch of monotonic region is not considered to be a feature because it doesn’t give us much information. This is in contrast with a sharp corner or a unique color combination, where we get a lot of information about the image. We can use the BoW model for image classification by constructing a large vocabulary of many visual words and represent each image as a histogram of the frequency words that are in the image. To actually use BoW for image classification, we need to extract these features, generate a codebook, and then generate a histogram. in order to get these features we are going to Use Local Features Descriptors like ( SIFT , SURF ,ORB )\\
After this step, each image is a collection of vectors of the same dimension, where the order of different vectors is\textbf{ not important} , but the sets vary in cardinality and lack meaningful ordering. This creates difficulties for learning methods (e.g., classifiers) that require feature vectors of fixed dimension as input.\\ Use of the \textbf{vector quantization (VQ)} technique clusters the keypoint descriptors in their feature space into a large number of clusters using \textbf{the K-means } clustering algorithm and encodes each keypoint by the index of the cluster to which it belongs.\\ Each cluster as a visual word that represents a specific local pattern shared by the keypoints in that cluster is saved. Thus, the clustering process generates a visual word vocabulary describing different local patterns in images. The number of clusters determines the size of the Codebook , Hence, each patch in an image is mapped to a certain codeword through the clustering process.

\subsection{keypoint descriptor Clustering }
A texture image with a Bag of Words
model is represented by the image feature,i.e.,a histogram of
the number of texels from a texture image, to generate ”visual
words”, which generated word Code, statistics of each image
frequency of visual words, to complete the image of the word
description of the bag. Fig.\ref{fig:bag} is bag of visual words image
classiﬁcation approach.\\
\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{img/BAG.jpg}
\caption{ bag of visual words image classification approach   }
\label{fig:bag}
\end{figure}
(1) \textbf{Feature extraction and description.} It is to extract a representative local features from an image as a description   \\(2)\textbf{ Dictionary}
generation. How to generate the visual dictionary in the
entire feature space. After K-means clustering SURF 
feature will be clustered into K categories,\\(3)\textbf{ Characteristics of quantization.} During
quantization characteristic, for a given local features of each
image, using the nearest neighbor method to find the distance
to the nearest cluster center feature vectors in the visual
dictionary, the resulting image should be the corresponding
visual words to represent the image of the gesture.


\subsection{classification algorithm }

Training Processes are as follows:\\ (1) extracting SURF
features from training samples with different
scales and  orientations .
\\ (2) using K-means to cluster Surf features into K centroids ,  Once this is done, each feature vector (keypoint) is
assigned to one and only one cluster center that is in the
nearest distance with respect to the Euclidean metric in 64
dimensional feature vectors. The keypoints that are assigned to
the same cluster center will be in the same subgroup so that
after clustering we have k disjoint subgroups of keypoints.
Therefore k-means clustering decreases dimensionality for
every training image with n keypoints (n $\times$ 64) to 1 $\times$ k,
where k is number of clusters.\\ (3) calculating each figure as training visual saliency
map, and each image in the Surf features to quantify, and
then based on visual saliency visual computing word weighted
histogram of each graph, and then use the histogram training
image.\\ (4)  SVM uses a non-linear mapping to transform original training dataset into higher dimension and searches for a linear optimal
separating hyperplane. since our Gesture can have the same shape and certainly often have the same features for two different gestures  this will make Linear separation Hard and can results in false results the kernel method can be used in our case  to transform data  in to higher dimension space in which they are separable. This process will allow classification of multiple classes (16 class ) 

\\validation :
we randomize our data 10 times ...


\\ Classiﬁcation
process:\\ (1) The test image used to extract dense sampling of
images features. \\(2) Calculation of the test image visual words
and quantify the test image, and then calculate the saliency
calculation test image visual word weighted histogram.\\ (3)
using the trained SVM classiﬁer to obtain a classiﬁcation
result.
the following Figure \ref{fig:algo1}  summarize the whole  process :

\begin{figure}[!h]
\centering
\includegraphics[width=17cm, height=19.5cm ]{img/myalgo.pdf}
\caption{Flowchart for the structure of the algorithm , in block (A) the Extracted Keypoints  are Clustered to i Vocabulary and generate a codewords dictionary  ,then compute a histogram using visual words ( vocabulary ) for all the training samples these histograms are then fed to an SVM for classification, in Block (B) test images are converted to histogram representation by couting how many keypoints appeared in cluster i ,  and then classify the results using the trained SVM }
\label{fig:algo1}
\end{figure}

\newpage

for Surf Descriptor we have 200 image for each Gesture  which gave approximately 400444 Features to clusters 

\subsection{EXPERIMENTAL RESULTS }
in this section we are interested in maximizing the performances of SVM and KNN , using K-cross Validation to make model selection , using K = 10 for cross Validation which has ensure us a great    ,  . \\
we have chose Confusion to be our main Evaluation Metric since The method can be used for binary classification or Multi class classifiction like Our Case ( 16 class ) 

\subsubsection{ } 

confusion mtrix ...\\
SVM performance ...

\newpage




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{gesture recognition using Fourier Descriptor and NN  classifier }

 The use of depth image has benefit
in segmenting hand image rather than color-based
segmentation, segmentation in depth image is more robust
since the lighting variation does not affect the image quality.
 Fourier descriptors represent feature of each hand gesture as Unique signature As long as the shape (images ) are different from each others .
Generally, this research is separated into two phases:
dictionary build phase and classification phase. \\In the
dictionary build phase, the Fourier descriptors of each
character are stored into a database to develop gesture
dictionary. The gesture dictionary and comparison methods are
derived from \cite{clif}.\\ The classification phase has the similar
step, except that the Fourier descriptors are compared with the
dictionary using distance metric as classification methods.
The result of classification phase is the meaning of the
acquired gesture sign.\\\\
\texttt{
For more details about the algorithm used for this project please check the section \ref{FDT}
}

\subsection{Classification }
Now we have a set of features for each image and the next
step is to train a classifier to classify the different hand shapes
used in testing. the classifier used in for Fourier descriptor is based on Euclidean distance which is a very simple way for
classification.\\
Classification using Euclidean Distance: The set of
features induces a distance on the shape space, which is given
by the Euclidean distance: 
$$d(f_{1},f_{2}) = \sqrt{\sum_{k=0}^{N-1} |f_{1}(k)-f_{2}(k)|^{2}}$$\\
\texttt{ Where $f_{1},f_{2}$ are the feature vectors for the two images being
compared. The two vectors with the least distance will have
the same class}\\



 The following Figure \ref{fig:dft} summarize the whole process :  
        
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{img/latex.jpg}
\caption{the architecture used for FDs}
\label{fig:dft}
\end{figure}


\subsection{EXPERIMENTAL RESULTS}

For each shape we select a set of points with the equal point
sampling method. To use DFT we choose a number that is a
power-of-two. We tested the classifiers with initial 128, 64, 32, 16
and 8 points. Those features are used as a training set to be
used in classification. To classify the test image we used the Classifier Nearest Neighbor that uses simple  Euclidean distance , where we calculated the distance between the features of each image in
the test set with the features of the Dictionary .

%recognition rate for each classifier using the formula in \ref{equa:1}.\\
%\begin{gather}
 %   Recognition\ rate = \frac{the\ number\ of\ recognized\ images}{the\ number\ of\ %testing\ images} 
  %  \label{equa:1}
%\end{gather}